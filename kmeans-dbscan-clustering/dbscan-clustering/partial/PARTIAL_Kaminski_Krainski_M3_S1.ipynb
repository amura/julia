{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make sure to install all required packages.\n",
    "# You can do it by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ]add Arrow CSV DataFrames Plots Clustering Distances FreqTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you launched Jupyter in directory with attached Project.toml and Manifest.toml\n",
    "# use below command to install required packages with fixed versions. \n",
    "# Check Project introduction for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ] instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "using Arrow\n",
    "using CSV\n",
    "using DataFrames\n",
    "using Plots\n",
    "using Clustering\n",
    "using Distances\n",
    "using Random\n",
    "using Statistics\n",
    "using FreqTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read clean dataset from Arrow file into DataFrame\n",
    "sales_norm = DataFrame(***(\"sales_norm.arrow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to Matrix and make sure that one product is one column in the resulting matrix\n",
    "cluster_data = ***(sales_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Euclidean distances between all points\n",
    "dists = pairwise(SqEuclidean(), cluster_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify epsilons and minpts range\n",
    "eps_rng = ***:***:***\n",
    "minpts_rng = ***:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate average silhouettes values, number of clusters and mean clusters size \n",
    "#for each combination of hyperparameters\n",
    "avg_shts_m = Array{Float64}[]\n",
    "no_clust_m = Array{Int64}[]\n",
    "avg_clust_size_m = Array{Float64}[]\n",
    "for m in minpts_rng\n",
    "    avg_shts = Float64[]\n",
    "    no_clust = Int64[]\n",
    "    avg_clust_size = Float64[]\n",
    "    for e in eps_rng\n",
    "        clustering = ***(dists, e, m)\n",
    "        cluster_ids = ***\n",
    "        if ***(cluster_ids) <= 1\n",
    "            silh = ***\n",
    "        else\n",
    "            idx = cluster_ids .!=0\n",
    "            silh = ***(***(cluster_ids[idx], dists[idx, idx]))\n",
    "        end\n",
    "        push!(avg_shts, silh)\n",
    "        push!(no_clust, ***(***))\n",
    "        push!(avg_clust_size, ***(***))\n",
    "    end\n",
    "    push!(avg_shts_m, avg_shts)\n",
    "    push!(no_clust_m, no_clust)\n",
    "    push!(avg_clust_size_m, avg_clust_size)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average silhouettes values\n",
    "plot(***, ***, xlab = \"epsilon\", ylab = \"Mean silhouette value\", \n",
    "    labels = permutedims([\"m=\"*string(m) for m in 1:6]), linewidth = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot number of clusters\n",
    "plot(***, ***[***], xlab = \"epsilon\", ylab = \"Number of clusters\", \n",
    "    labels = permutedims([\"m=\"*string(m) for m in 2:6]), linewidth = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average clusters size\n",
    "plot(***, ***[***], xlab = \"epsilon\", ylab = \"Average cluster size\",\n",
    "        labels = permutedims([\"m=\"*string(m) for m in 2:6]), linewidth = 2, legend=:topleft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN is producing really small clusters up to 40.0 value of epsilon - we want to have few clusters we can analyze and write down the findings. Getting back to the average silhouette plot, we can se that for epsilon > 40.0 there is a spike around epsilon equal to 50.0 for m=5 or m=6. \n",
    "\n",
    "We'll use epsilon=50.0 and m=6 as hyperparameter values for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce final DBSCAN clustering\n",
    "opt_clustering = ***(dists, 50.0, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the members count in each cluster\n",
    "# There are 458 products considered as outliers\n",
    "#observations are dispersed in high dimensional space and don't create dense areas\n",
    "***(opt_clustering.assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot clusters averages (also include outliers as separate cluster)\n",
    "# Each cluster has some distinct characteristic we should summarize for the recipients of the report\n",
    "plot(hcat([mean(cluster_data[:, *** .== ***], dims=2) for i in 0:2]...), \n",
    "    xlab=\"Week\", ylab=\"Normalized sales\", labels=[0 1 2], linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot clusters standard deviations\n",
    "# Variability visibly differs between groups\n",
    "plot(hcat([std(cluster_data[:, *** .== ***], dims=2) for i in 0:2]...), \n",
    "    xlab=\"Week\", ylab=\"Normalized sales\", labels=[0 1 2], linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load k-means assignments from Milestone 2\n",
    "kmeans_assignments = ***.(Int64,***(\"kmeans_assignments.txt\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce two-way table for DBSCAN and k-means assignments\n",
    "# As we can see, Cluster 2 from DBSCAN consists almost only of observations from k-means Cluster 4\n",
    "# Outliers and Cluster 1 from DBSCAN have no clear mapping to k-means while considering observations membership \n",
    "#(but we still see similar shape on average sales plot between DBSCAN Cluster 1 and k-means Cluster 1) \n",
    "***(opt_clustering.assignments,kmeans_assignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of clustering results**\n",
    "\n",
    "Based on DBSCAN clustering evaluation we produced *** clusters with *** observations in total (we removed *** products as outliers).\n",
    "\n",
    "* Cluster 1 ***\n",
    "* Cluster 2 ***\n",
    "* Outliers ***\n",
    "\n",
    "Variance analysis shows that ***\n",
    "\n",
    "We produced two distinct clusterings using `k-means` and `DBSCAN` methods. *** . Considering results from both clustering methods we can also see similar patterns for particular clusters:\n",
    "* DBSCAN Cluster 1 is similar to ***\n",
    "* DBSCAN Cluster 2 is alike ***\n",
    "\n",
    "Such similiarties strengthen our confidence in quality and meaningfulness of the results. Both clustering results may be relevant for product team and person with in-depth knowledge about the products may provide additional comments regarding clustering quality. \n",
    "\n",
    "As a final technical detail, we want to highlight that our goal was to spot distinct groups in the product base regardless of the density of these groups. DBSCAN catch dense regions well, but related products are not guaranteed to be close to one another. We consider k-means to be our main analysis technique and DBSCAN as a validation tool."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
